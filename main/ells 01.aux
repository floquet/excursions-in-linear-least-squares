\relax 
\@writefile{toc}{\contentsline {chapter}{Preface}{xi}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Life is complex}{xi}}
\@writefile{lot}{\contentsline {table}{\numberline {0.1}{\ignorespaces Matrix manipulations for $\A {*}$ and $\A {T}$.}}{xi}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Readings}{xi}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Least Squares}{xi}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Linear Algebra and Matrix Analysis}{xii}}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Numerical Linear Algebra}{xii}}
\@writefile{toc}{\contentsline {section}{\numberline {0.6}Discussions on Least Squares}{xii}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{I}Rudiments}{1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Least Squares Problems}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Linear Systems}{3}}
\newlabel{eq:axeb}{{1.1}{3}}
\newlabel{eq:basics}{{1.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}$ \left \delimiter 69645069    \A {}\tmspace  +\thinmuskip {.1667em}x - b  \right \delimiter 86422285  = 0$}{3}}
\newlabel{eq:axmb}{{1.1.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}$ \left \delimiter 69645069    \A {}\tmspace  +\thinmuskip {.1667em}x - b  \right \delimiter 86422285  > 0$}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Least Squares Solutions}{4}}
\newlabel{eq:simple case}{{1.3}{4}}
\newlabel{eq:xstar}{{1.4}{4}}
\newlabel{eq:simple case solution}{{1.5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The residual error $\left \delimiter 69645069 r \right \delimiter 86422285 _{2}$ given in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 1.5\hbox {}\unskip \@@italiccorr )}}.}}{5}}
\newlabel{fig:v graph}{{1.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Zonal Approximation}{5}}
\newlabel{ssec:zonal solution}{{1.2.1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{Zonal Problem}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Scalar function $\phi $ and approximations.}}{6}}
\newlabel{fig:sticks}{{1.2}{6}}
\newlabel{eq:zonalls}{{1.6}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Zonal Solution}{6}}
\newlabel{ssec:modal approx}{{1.2.2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Modal Approximation}{7}}
\newlabel{ssec:modal problem}{{1.2.2}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Modal Problem}{7}}
\newlabel{ssec:modal solution}{{1.2.2}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Modal Solution}{7}}
\newlabel{eq:modalls}{{1.7}{7}}
\newlabel{eq:column vectors}{{1.8}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Errors}{8}}
\newlabel{sec:lsp}{{1.3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}General Least Squares Problem and Solution}{8}}
\newlabel{eq:xlsdef}{{1.9}{8}}
\newlabel{eq:general soln}{{1.10}{8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Least Squares Theory}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Fundamental Theorem of Linear Algebra}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The Fundamental Theorem of Linear Algebra}}{10}}
\newlabel{tab:ftola}{{2.1}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The Fundamental Theorem of Linear Algebra in pictures}}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Dimensions of the fundamental subspaces for $ \A {} \in \mathbb  {C}^{m \times n}_{\rho }$.}}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Singular Value Decomposition\ - I}{12}}
\newlabel{ssec:SVD Theorem}{{2.2.1}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}SVD Theorem}{13}}
\newlabel{eq:svdk}{{2.1}{13}}
\newlabel{eq:svd block}{{2.2.1}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Orthonormal spans for the invariant subspaces.}}{13}}
\newlabel{SVD and Least Squares}{{2.2.2}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}SVD and Least Squares}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Unitary transformation}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Pseudoinverse solution}{14}}
\newlabel{eq:svd soln}{{2.2.2}{15}}
\newlabel{eq:mpptsvd}{{2.2.2}{15}}
\newlabel{eq:r2ub}{{2.2.2}{15}}
\@writefile{toc}{\contentsline {subsubsection}{In retrospect}{15}}
\newlabel{eq:r2:a}{{2.4}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Decomposing the data vector.}}{16}}
\newlabel{fig:decomposing data vector}{{2.1}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Singular Value Decomposition\ - II}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}$ \Sigma ^{\mathrm  {}} $ Gymnastics}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Fundamental Projectors}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Fundamental Projectors using the pseudoinverse.}}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Fundamental Projectors using domain matrices.}}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Projections of the data vector.}}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Least Squares Solution - Again}{19}}
\newlabel{eq:}{{2.4}{19}}
\newlabel{sec:reflection invariance}{{2.5}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Reflection Invariance}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Data vector resolved as $b = {\color {blue} {b_{ \mathcal  {R} }}} + {\color {red} {b_{ \mathcal  {N} }}}$.}}{20}}
\newlabel{fig:null up}{{2.3}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Data vector resolved as $b + {\color {red} {b_{ \mathcal  {N} }}} = {\color {blue} {b_{ \mathcal  {R} }}}$.}}{20}}
\newlabel{fig:null down}{{2.4}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces Reflecting the data for $\A {}x - T = r$.}}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Reflecting the data points through the solution curve.}}{21}}
\newlabel{fig:learn:reflect}{{2.5}{21}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Playing With Lines}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Given the parameters for a line find the solution locus.}}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}A Single Line}{23}}
\newlabel{eq:myline}{{3.1}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A sample line.}}{24}}
\newlabel{eq:one line problem}{{3.2}{24}}
\newlabel{eq:one line merit}{{3.3}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Geometric Solution}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Problem statement for least squares solution for a single line.}}{25}}
\newlabel{tab:one line inputs}{{3.2}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Every point on the line satisfies the least squares criterion; the particular solution is the vector of minimum length.}}{25}}
\newlabel{fig:one line fan}{{3.2}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The point on the line closest to the origin.}}{26}}
\newlabel{fig:one line osculate}{{3.3}{26}}
\newlabel{eq:one line osculating}{{3.4}{26}}
\newlabel{eq:one line full soln}{{3.5}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The least squares solution for \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 3.2\hbox {}\unskip \@@italiccorr )}} resolved into range space (blue) and null space components (red).}}{27}}
\newlabel{fig:one line resolved}{{3.4}{27}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Results for best line with $a_{0} = \frac  {1}{2}$, $a_{1} = -\frac  {1}{2}$.}}{27}}
\newlabel{tab:one line solution}{{3.3}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Solution Space}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A contour plot of the merit function showing the particular solution (blue dot) and homogeneous solution (red dashes).}}{28}}
\newlabel{fig:one lines merit}{{3.5}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}SVD}{28}}
\@writefile{toc}{\contentsline {subsubsection}{$ \mathbf  {U}^{\mathrm  {  }} $: codomain matrix}{28}}
\@writefile{toc}{\contentsline {subsubsection}{$\V {}$: domain matrix}{28}}
\citation{Laub2005}
\@writefile{toc}{\contentsline {subsubsection}{$ \textbf  {S}^{\mathrm  {  }} $: matrix of singular values }{29}}
\@writefile{toc}{\contentsline {subsubsection}{Assemble components}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Construct pseudoinverse}{29}}
\newlabel{eq:vectorpi}{{3.7}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Solve the least squares problem}{29}}
\newlabel{eq:one line soln}{{3.8}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Two Lines}{30}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Existence and uniqueness with two lines.}}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Crossing Lines}{30}}
\newlabel{eq:crossing}{{3.10}{30}}
\@writefile{toc}{\contentsline {subsubsection}{Troubling Sequences}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Parallel Lines}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Parallel lines and the least squares solution.}}{32}}
\newlabel{fig:tantalizing}{{3.6}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Intersecting Lines}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Overlapping Lines}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Three Lines}{33}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Rewriting the equations $y=mx+b$ as a linear system.}}{33}}
\newlabel{eq:p(m)}{{3.11}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Trajectory of the solution point $p(m)$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 3.11\hbox {}\unskip \@@italiccorr )}} for $-\infty < m < \infty $.}}{34}}
\newlabel{fig:three lines oval}{{3.7}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Least squares solution for three distinct lines as the parameter $m$ varies from 0 to $\infty $.}}{35}}
\newlabel{tab:three lines graphs}{{3.6}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Least squares solution for three distinct lines as the parameter $m$ varies from 0 to $\infty $.}}{36}}
\newlabel{tab:three lines merit}{{3.7}{36}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{II}Modal Example}{37}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Solution via Calculus}{41}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Modal Approximation}{41}}
\newlabel{eq:lr trial}{{4.1}{41}}
\newlabel{eqn:merit}{{4.1}{41}}
\citation{Bevington}
\newlabel{eq:gradient lr}{{4.2}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Bevington Example}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Measuring the temperature of a bar.}}{42}}
\newlabel{fig:bar}{{4.1}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Problem Statement}{42}}
\newlabel{sec:normal I}{{4.2.2}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Normal Equations via Calculus}{43}}
\newlabel{eq:bev pde}{{4.3}{43}}
\newlabel{eq:calculus normal}{{4.4}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Problem statement for linear regression.}}{44}}
\newlabel{tab:bevington inputs}{{4.1}{44}}
\newlabel{eq:modal matrix eq}{{4.5}{44}}
\newlabel{eq:det}{{4.6}{44}}
\newlabel{eq:bevington matrix inverse}{{4.7}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Raw data and results.}}{45}}
\newlabel{tab:bevington data and results}{{4.2}{45}}
\newlabel{eqn:bevington solution product}{{4.2.2}{45}}
\newlabel{eqn:bevington soln sum}{{4.8}{45}}
\newlabel{eqn:bevington error terms}{{4.2.2}{45}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Results for linear regression.}}{46}}
\newlabel{tab:bevington solution}{{4.3}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Numerical Results}{46}}
\newlabel{sec:exact form}{{4.3.1}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Exact Form}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Computed Form}{47}}
\newlabel{eq:soln vector}{{4.3.2}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Visualization}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Seeing the Solution}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Data v. Fit}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Solution plotted against data with residual errors shown in red.}}{49}}
\newlabel{fig:bevington soln v data}{{4.2}{49}}
\@writefile{toc}{\contentsline {subsubsection}{Residual Errors}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Scatter plot of residual errors.}}{50}}
\newlabel{fig:bevington residuals}{{4.3}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Scatter plot of residual errors with data points connected.}}{50}}
\newlabel{fig:bevington residuals line}{{4.4}{50}}
\@writefile{toc}{\contentsline {subsubsection}{Solution v. Minimum}{50}}
\newlabel{fig:bevington merit}{{4.4.1}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The merit function.}}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Seeing the Uncertainty}{51}}
\newlabel{fig:bevington residuals lines}{{4.4.1}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Another look at the merit function showing the primary error ellipse (black) and contour levels (gray).}}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Digging Deeper}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Whisker plot showing 250 randomly sampled solutions.}}{53}}
\newlabel{fig:bev whisker}{{4.7}{53}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces The solution parameters expressed as normal distributions.}}{53}}
\newlabel{tab:dev normal}{{4.4}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Scatter plot showing sampling of solutions.}}{54}}
\newlabel{fig:bev scatter}{{4.8}{54}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Comparing samples to ideal normal distribution.}}{54}}
\newlabel{tab:bev census}{{4.5}{54}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Solution via SVD}{57}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Computing the SVD}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Singular values}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Matrix for the domain}{58}}
\newlabel{eq:decompwx}{{5.1}{58}}
\newlabel{eq:vrot}{{5.2}{59}}
\newlabel{eq:costtheory}{{5.3}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Matrix for the codomain}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Error terms}{60}}
\newlabel{eq:lr errors}{{5.1.4}{60}}
\newlabel{sssec:archetype viz}{{5.2}{61}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Visualization}{61}}
\newlabel{eq:alpha}{{5.2}{62}}
\newlabel{eq:white arrow}{{5.4}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The solution vector is the mixture of $u_{1}$ and $u_{2}$ which eliminates error.}}{63}}
\newlabel{fig:u min}{{5.1}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Measurement space for the Bevington example}}{64}}
\newlabel{fig:bevington codomain}{{5.2}{64}}
\newlabel{eq:archetype:data vector}{{5.2}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Minimization occurs in the codomain.}}{65}}
\newlabel{fig:bevington codomain with data}{{5.3}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Data vector $T = {{\color {blue} {T_{ \mathcal  {R} }}}} + {{\color {red} {T_{ \mathcal  {N} }}}}$ resolved into range and null space components as in figure 2.3\hbox {}.}}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Decomposing $\left \delimiter 69645069 {\color {red} {r}} = {\color {red} {T_{ \mathcal  {N} }}} \right \delimiter 86422285 _{2}^{2}$ into residual error terms $r_{k}^2$ of table 5.3\hbox {}.}}{65}}
\newlabel{tab:bevington poles}{{5.2}{66}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces The column vectors of $ \mathbf  {U}^{\mathrm  {  }} $.}}{66}}
\newlabel{tab:bevington usv block}{{5.2}{66}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Singular value decomposition for the system matrix $\A {}$.}}{66}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces A summary of the residual errors and their contributions to $\left \delimiter 69645069 {\color {red} {r}} \right \delimiter 86422285 _{2}$.}}{67}}
\newlabel{tab:bev r decomposition}{{5.3}{67}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Solution Via Other Methods}{69}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:normal II}{{6.1}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Normal Equations from Vectors}{69}}
\newlabel{eq:bevington axeb}{{6.1}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Composing the normal equations}{70}}
\newlabel{eq:det again}{{6.2}{71}}
\newlabel{eq:bevington:soln:vectors}{{6.3}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}$ \textbf  {Q}^{\mathrm  {  }}  \textbf  {R}^{\mathrm  {  }} $ Decomposition}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Computing the QR Decomposition}{72}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Finer Points}{75}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Invariances}{75}}
\newlabel{spec:translation invariance}{{7.1.1}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Translation Invariance}{75}}
\newlabel{sssec:translation}{{7.1.1}{76}}
\@writefile{toc}{\contentsline {subsubsection}{Translation}{76}}
\newlabel{eq:translation}{{7.1}{76}}
\@writefile{toc}{\contentsline {subsubsection}{Demonstrations}{76}}
\newlabel{eq:translation:dot products}{{7.2}{76}}
\@writefile{toc}{\contentsline {subsubsection}{Computations}{76}}
\newlabel{eq:bev:prediction}{{7.3}{77}}
\@writefile{toc}{\contentsline {subsubsection}{Visuals}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Solution after translation along $x$ axis: the slope is invariant.}}{77}}
\newlabel{fig:bevington translate soln v data}{{7.1}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Reflection Invariance}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Scatter plot of residual errors.}}{78}}
\newlabel{fig:bevington translate residuals}{{7.2}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Fitting To Higher Orders}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Removing Terms}{78}}
\newlabel{eq:difference jk}{{7.3}{78}}
\newlabel{eq:difference jk}{{7.3}{78}}
\newlabel{fig:bevington translate merit}{{7.1.1}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The merit function after translation.}}{79}}
\newlabel{eq:rule 1}{{7.3}{79}}
\newlabel{eq:rule 2}{{7.3}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces The merit function for the learning curve showing the minimum and the value.}}{80}}
\newlabel{fig:learn:merit}{{7.4}{80}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Relating the pair index $\mu $ to the indices $j$ and $k$.}}{80}}
\newlabel{default}{{7.1}{80}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{III}Zonal Example}{81}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Zonal Example}{83}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:archetype zonal}{{8}{83}}
\newlabel{sec:zonal problem}{{8.1}{83}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Problem}{83}}
\newlabel{ssec:zonal subsection}{{8.1.1}{83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Zonal Subsection}{83}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Lines}{85}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Face-centered cubic lattice}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces A slice of a face-centered cubic lattice showing a single crystal.}}{85}}
\newlabel{fig:pattern fcc}{{9.1}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Simulation output showing atomic shades shaded by potential energy.}}{86}}
\newlabel{fig:pattern potentials}{{9.2}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Model}{86}}
\newlabel{eq:trial coupled}{{9.1}{86}}
\newlabel{eq:system}{{9.2}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Full data set showing inset.}}{87}}
\newlabel{fig:simulation inset}{{9.3}{87}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Sample data set showing fit parameters.}}{87}}
\newlabel{fig:parameters}{{9.4}{87}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Data sets and basic results}}{88}}
\newlabel{tab:results grains}{{9.1}{88}}
\newlabel{eq:lines A}{{9.3}{89}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Solution}{89}}
\newlabel{eq:mnormal}{{9.3}{89}}
\newlabel{eq:det}{{9.3}{89}}
\newlabel{eq:rhs}{{9.3}{90}}
\newlabel{eq:soln3}{{9.3}{90}}
\newlabel{eq:error3}{{9.3}{90}}
\newlabel{eq:errors2}{{9.3}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Problem Statement}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Data}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Results}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.1}Least Squares Results}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.2}Apex Angles}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.3}Qualitative Results}{90}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Problem statement for grain identification by rows (coupled linear regression).}}{91}}
\newlabel{tab:find lines problem statement}{{9.2}{91}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Solutions for three data sets.}}{91}}
\newlabel{fig:three lines}{{9.5}{91}}
\@writefile{lot}{\contentsline {table}{\numberline {9.3}{\ignorespaces Point membership in data sets shown in figure 9.1\hbox {}.}}{92}}
\newlabel{tab:tres rows}{{9.3}{92}}
\@writefile{lot}{\contentsline {table}{\numberline {9.4}{\ignorespaces Excerpted data set.}}{93}}
\@writefile{lot}{\contentsline {table}{\numberline {9.5}{\ignorespaces Least squares results for three axes.}}{93}}
\@writefile{lot}{\contentsline {table}{\numberline {9.6}{\ignorespaces Intermediate results: angles for the axes.}}{93}}
\@writefile{lot}{\contentsline {table}{\numberline {9.7}{\ignorespaces Final results: apex angle measurements}}{93}}
\newlabel{tab:apex angles}{{9.7}{93}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Apex angles displayed in table 9.7\hbox {}.}}{94}}
\newlabel{fig:pattern needles}{{9.6}{94}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces Merit functions for the three data sets.}}{95}}
\newlabel{fig:3 bullseyes}{{9.7}{95}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Crystals}{97}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Stitching Local Maps}{99}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}What is stitching?}{99}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Stitching local maps together to form a global map.}}{99}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Stitch $\phi $}{99}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Genesis}{99}}
\newlabel{eq:phi}{{11.2.1}{99}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.2}Data}{99}}
\@writefile{lot}{\contentsline {table}{\numberline {11.1}{\ignorespaces The input data in continuous and discrete form.}}{100}}
\@writefile{lot}{\contentsline {table}{\numberline {11.2}{\ignorespaces Sample showing two zones with overlap.}}{100}}
\newlabel{tab:stitch raw data}{{11.2}{100}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces The ideal potential function showing five measurement zones and four overlap bands.}}{101}}
\newlabel{fig:example}{{11.2}{101}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Waterfall diagram showing discretization within measurement zones with left and right zone overlaps.}}{101}}
\newlabel{fig:dots and overlaps}{{11.3}{101}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Stitching unifies the data.}}{102}}
\newlabel{fig:stitch final}{{11.4}{102}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces A set of piston adjustments which restores continuity across the domain.}}{102}}
\newlabel{fig:pistons}{{11.5}{102}}
\@writefile{lot}{\contentsline {table}{\numberline {11.3}{\ignorespaces Measurements displaying the connection between overlap bands in figure 11.3\hbox {}.}}{103}}
\newlabel{tab:stitch measurements}{{11.3}{103}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.3}Data and results}{103}}
\@writefile{lot}{\contentsline {table}{\numberline {11.4}{\ignorespaces Computation of the zone shift values.}}{103}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.4}Linear System}{103}}
\@writefile{lot}{\contentsline {table}{\numberline {11.5}{\ignorespaces Computation of the zone shift values.}}{104}}
\@writefile{lot}{\contentsline {table}{\numberline {11.6}{\ignorespaces Input data}}{104}}
\newlabel{tab:stitch piston in}{{11.6}{104}}
\newlabel{eq:stitch linear system}{{11.2.4}{104}}
\newlabel{eq:stitch general}{{11.2.4}{104}}
\@writefile{lot}{\contentsline {table}{\numberline {11.7}{\ignorespaces Problem statement for linear regression.}}{105}}
\newlabel{tab:stitching problem statement}{{11.7}{105}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.5}Least Squares Arbitration}{105}}
\@writefile{lot}{\contentsline {table}{\numberline {11.8}{\ignorespaces Results for stitching with piston.}}{106}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces Looking at the merit function on the $p_{2} - p_{3}$ axis.}}{107}}
\newlabel{fig:merit}{{11.6}{107}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces Pistons from the solution and pistons used to create the data}}{108}}
\newlabel{fig:pistons plus}{{11.7}{108}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Stitch $\nabla \phi $}{108}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces A set of tilt adjustments which restores continuity of the gradient across the domain.}}{109}}
\newlabel{fig:tilts}{{11.8}{109}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces A function and its gradient.}}{109}}
\newlabel{fig:function and gradient}{{11.9}{109}}
\newlabel{eq:phi}{{11.3}{109}}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Gradient I}{111}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}One Dimension}{111}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{IV}Applications:\\Nonlinear Problems}{113}}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Finding the Best Circle}{117}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Model}{117}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}Problem Statement}{117}}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Data}{117}}
\@writefile{toc}{\contentsline {section}{\numberline {13.4}Example}{117}}
\@writefile{lot}{\contentsline {table}{\numberline {13.1}{\ignorespaces Problem statement for linear regression.}}{118}}
\newlabel{tab:bevington inputs}{{13.1}{118}}
\@writefile{lot}{\contentsline {table}{\numberline {13.2}{\ignorespaces Results for best circle}}{119}}
\newlabel{tab:results census}{{13.2}{119}}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Linearization}{121}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Powers Laws and Exponentials}{121}}
\newlabel{eq:learning curve}{{14.1}{121}}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Learning Curve}{121}}
\newlabel{eq:learn merit surface}{{14.2}{121}}
\newlabel{tab:data learn}{{14.2}{122}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.1}Problem Statement}{122}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces Merit function for the learning curve in solution space.}}{123}}
\newlabel{fig:learn merit naked}{{14.1}{123}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.2}Solution}{123}}
\newlabel{eq:learn:a}{{14.2.2}{123}}
\@writefile{lot}{\contentsline {table}{\numberline {14.1}{\ignorespaces Problem statement for learning curve.}}{124}}
\newlabel{tab:learn:problem statement}{{14.1}{124}}
\@writefile{lot}{\contentsline {table}{\numberline {14.2}{\ignorespaces The simultaneous conditions defining $\nabla M(a,b) = 0$.}}{124}}
\newlabel{tab:learn:gradient}{{14.2}{124}}
\newlabel{eq:learn:merit 1d}{{14.3}{124}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces Merit function constrained to one parameter $b$.}}{125}}
\newlabel{fig:learn:one d}{{14.2}{125}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.3}Results}{125}}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}What Not To Do}{125}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.1}Logarithmic Transform}{125}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces Merit function for the learning curve in solution space showing the constrained $a$ parameter as a dotted line.}}{126}}
\newlabel{fig:learn:constrained merit}{{14.3}{126}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.4}{\ignorespaces Solution for equations \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 14.2\hbox {}\unskip \@@italiccorr )}} and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 14.3\hbox {}\unskip \@@italiccorr )}} using data in table 14.2\hbox {}.}}{126}}
\newlabel{fig:learn: data v soln}{{14.4}{126}}
\@writefile{lot}{\contentsline {table}{\numberline {14.3}{\ignorespaces Results for learning curve analysis.}}{127}}
\newlabel{tab:bevington solution}{{14.3}{127}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.5}{\ignorespaces The residual errors in figure 14.4\hbox {}.}}{127}}
\newlabel{fig:learn:residuals}{{14.5}{127}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.6}{\ignorespaces Minimization of the merit function for the learning curve.}}{128}}
\newlabel{fig:learn:merit}{{14.6}{128}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.2}Linear Transformation}{128}}
\newlabel{eq:learn:soln:faux}{{14.4}{129}}
\newlabel{eq:learn:merit:min}{{14.5}{129}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.7}{\ignorespaces The merit function for the learning curve showing the minimum and the value .}}{129}}
\newlabel{fig:learn:merit}{{14.7}{129}}
\newlabel{ssec:RTF}{{14.3.3}{130}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.3}Reflection Test Fails}{130}}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}Radioactive Decay}{130}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.1}Theory}{130}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.2}Problem Statement}{130}}
\@writefile{lot}{\contentsline {table}{\numberline {14.4}{\ignorespaces Problem statement for radioactive decay.}}{130}}
\newlabel{tab:bevington inputs}{{14.4}{130}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.3}Results}{130}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.8}{\ignorespaces The merit function for the radioactive decay model showing the minimum and the value .}}{131}}
\newlabel{fig:learn:merit}{{14.8}{131}}
\@writefile{lot}{\contentsline {table}{\numberline {14.5}{\ignorespaces Logarithmic scaling distorts errors.}}{132}}
\@writefile{lot}{\contentsline {table}{\numberline {14.6}{\ignorespaces Results for radioactive decay.}}{133}}
\newlabel{tab:bevington solution}{{14.6}{133}}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Population Growth}{135}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}Model}{135}}
\@writefile{lot}{\contentsline {table}{\numberline {15.1}{\ignorespaces Problem statement for population model with linear and exponential growth.}}{136}}
\newlabel{tab:census problem statement}{{15.1}{136}}
\@writefile{toc}{\contentsline {section}{\numberline {15.2}Problem Statement}{136}}
\newlabel{eq:census:error}{{15.3}{136}}
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Data}{136}}
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Example}{136}}
\@writefile{toc}{\contentsline {section}{\numberline {15.5}Polynomials}{136}}
\@writefile{lot}{\contentsline {table}{\numberline {15.2}{\ignorespaces Data v. prediction.}}{137}}
\newlabel{tab:census results}{{15.2}{137}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces Residual error for \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 15.3\hbox {}\unskip \@@italiccorr )}} with $a_{1}$, $a_{2}$, and $a_{3}$ at optimal values.}}{137}}
\newlabel{fig:census:line:residual error:wide}{{15.1}{137}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.2}{\ignorespaces Residual error for $a_{1}$ and $a_{2}$ fixed at optimal values.}}{138}}
\newlabel{fig:census:line:residual error:wide}{{15.2}{138}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.3}{\ignorespaces Solution plotted against data.}}{138}}
\newlabel{fig:census:data v soln}{{15.3}{138}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.4}{\ignorespaces Scatterplot of residual errors.}}{139}}
\newlabel{fig:census:scatterplot}{{15.4}{139}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.5}{\ignorespaces The merit function showing least squares solution.}}{139}}
\newlabel{fig:census:merit}{{15.5}{139}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.6}{\ignorespaces The merit function showing least squares solution and the null cline.}}{140}}
\newlabel{fig:census:merit}{{15.6}{140}}
\@writefile{lot}{\contentsline {table}{\numberline {15.3}{\ignorespaces Results for census analysis}}{140}}
\newlabel{tab:results census}{{15.3}{140}}
\@writefile{lot}{\contentsline {table}{\numberline {15.4}{\ignorespaces Fitting the census data with low order polynomials rb.}}{141}}
\@writefile{lot}{\contentsline {table}{\numberline {15.5}{\ignorespaces Fitting the census data with higher order polynomials.}}{142}}
\@writefile{lot}{\contentsline {table}{\numberline {15.6}{\ignorespaces Fitting the census data with low order polynomials.}}{143}}
\@writefile{lot}{\contentsline {table}{\numberline {15.7}{\ignorespaces Fitting the census data with higher order polynomials.}}{144}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.7}{\ignorespaces Total error $r^{\mathrm  {*}}r$ by order of fit.}}{145}}
\@writefile{lot}{\contentsline {table}{\numberline {15.8}{\ignorespaces Projections, by order of fit, for population in 2010.}}{145}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{V}Appendices}{147}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Exercises}{149}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Linear systems}{149}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}7.1.1\hbox {}}{149}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{VI}\ \ Backmatter}{151}}
\bibcite{Bellman1997}{1}
\bibcite{Bevington}{2}
\bibcite{Chan2007}{3}
\bibcite{Demmel1997}{4}
\bibcite{Golub1996}{5}
\bibcite{Higham2008}{6}
\bibcite{Horn1990}{7}
\bibcite{Horn1991}{8}
\bibcite{Mercer}{9}
\bibcite{Laub2005}{10}
\bibcite{Meyer2000}{11}
\bibcite{Strang2005}{12}
\bibcite{Trefethen2000}{13}
\bibcite{Eric}{14}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{153}}
